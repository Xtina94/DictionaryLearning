\documentclass[../Thesis.tex]{subfiles}

\begin{document}
\section{Introduction: the emerging field of graph signal processing}
Nowadays huge amounts of data are collected every day, from engineering sciences to our personal data regarding health monitoring, to financial data or political influences. The analysis and the process of these huge datasets has become a non indifferent challenge, since, in fact, data tends to reside on complex and irregular structures, which progressively required the introduction of innovative approaches to better handle and utilize them. \cite{Ortega2017} \cite{Sandry}

One of the central entities we will use in this work is the concept of \textit{graph signal}. Graphs are generic data representation structures, which are useful in a great variety of fields and applications for everyday life and technology in general. These structures are composed by two main elements: the nodes and the edges; the former are a set of points, identifying an aspect of the graph structure itself, while the latter are connections between the nodes. Several structures we can encounter in natural entities and abstract constructions can be represented by a graph structure, and when we associate values to their set of nodes and edges we obtain a graph signal. To be specific, a graph signal is seen as a finite collection of samples located at each node in the structure and which are interconnected among themselves through the edges, to which we can associate numerical values representing the weights of the connections. The metric for these weights is not unique, as it depends on which relation between the nodes we are looking at: a typical metric for graph weights may be, for example, inversely proportional to the distance (physical or not) between two different nodes, but other options are possible.

As previously mentioned, graph structures appear to be significantly helpful when they are used to represent signals and their applications are the most varied: we could focus on transportation networks, where we could want to work with data describing human migration patterns or trade goods; we could also apply graph theory over social networks, in which users can be seen, for example, as our nodes while their connections to friends are our edges. \cite{Ortega2017} Brain imaging is another interesting application of graph signals: through it we could infer the inner structure of some regions of the brain, in a way that permits us to better understand physiological dynamics.  \cite{Shuman2013} We could go on listing a considerable amount of valuable applications, from genetic and biochemical research to fundamental physical experiments; what these big amounts of data have in common is the fact that their complex inner structure makes it difficult to apply most of the well-known tools, like \gls{pca}, spectral analysis or \gls{svd}, without redefining the signal structure. \cite{Sandry}

\subsection{Mathematical description of the graph structure}
In the field of \gls{gsp}, spectral graph theory plays an important role, it focuses on analysing, constructing and manipulating graphs and makes uses of well know tools and concepts as frequency spectrum and the graph Fourier transform. In our work, the signals we are considering are defined on an undirected, connected, weighted graph $\mathcal{G} = \{ \mathcal{V}, \varepsilon, W\}$ consisting on a finite set of vertices $\mathcal{V}$ with $|\mathcal{V}| = N$, a set of edges $\varepsilon$ and a weighted adjacency matrix $\mathcal{W}$. The adjacency matrix contains the information of the connections between two nodes: if an edge $e$ is present between two vertices $i$ and $j$, then the entry $W_{i,j}$ represents the weight of that edge, while if there is no connection between two nodes, the value of the edge is null ($W_{i,j} = 0$). Moreover, if the graph was not connected and had K connected components, then we could decompose the graph signal over $\mathcal{G}$ into M sections which can be processed each one independently of the other.

Over this structure, we can then lay \cri{cerca un verbo migliore} the signal we need to analyse: to be precise, a signal or function $f: \mathcal{V} \to \R$ defined on the vertices of our graph, can be described as a vector $f \in \R^N$ that has the signal value at the $i^{th}$ vertex in $\mathcal{V}$ as the $i^{th}$ component of the vector $f$. \autoref{fig:sampleGraph} shows an example of graph signal: the colours of the vertices represents the values the signal has on them.

\begin{figure}
  \centering
  \includegraphics[width = .7\textwidth]{sampleGraph2.eps}
  \caption{Random sensor graph}
  \label{fig:sampleGraph}
\end{figure}
\subsection{the inverse covariance matrix}
\cri{Vedere se aggiungere o meno questa sezione}

\subsubsection{The graph Laplacian}
To go from the graph vertex domain (which plays the same role of the time domain in the classical signal processing theory) to the spectral domain the \textit{graph Laplacian operator}, or \textit{Combinatorial graph Laplacian}, has been introduced, which is defined as $\mathcal{L} := D - W$, where $D$ represents a diagonal matrix whose  $i^{th}$ diagonal element is equal to the sum of weights of all the edges incident to the vertex $i$. This entity is a difference operator, since it satisfies the condition:
\begin{equation}
((\mathcal{L} f)(i) = \sum_{j \in \mathcal{N}_i} W_{i,j}[f(i) - f(j)]
\end{equation}
The element $\mathcal{N}_i$ represents the set of vertices connected to vertex $i$ by an edge.

From its definition, the graph Laplacian $\mathcal{L}$ is a real symmetric matrix, thus it has a complete set of orthonormal eigenvectors, here denoted by $\{u_l\}_{l=0,1,\dots,N-1}$, to which real and non-negative eigenvalues are associated ($\{\lambda_l\}_{l=0,1,\dots,N-1}$). Together with the eigenvectors, they satisfy the condition:
\begin{equation}
\mathcal{L} u_l = \lambda_l u_l, \text{   for  } l = 0,1,\dots,N-1
\end{equation}
In the eigenvalues set, the one corresponding to $0$ has a multiplicity equal to the number of connected components of the graph; from this, since we are considering only connected graphs, we can assume the Laplacian eigenvalues have the distribution: $0 = \lambda_0 < \lambda_1 \leq \lambda_2 \dots \leq \lambda_{N-1} = \lambda_{max}$ and, of course, the set $\sigma(\mathcal{L}) = \{\lambda_0, \lambda_1,\dots,\lambda_{N-1}\}$ is the entire spectrum of our signal. \cri{trova un modo per usare la L polacca con questo font}

Eigenvalues and eigenvectors are then used to build up the graph version of a well known and useful transform, which is the \textit{classical Fourier Transform} and that we recall here being defined as:
\begin{equation}
\hat{f}(\xi) = \langle f, e^{2\pi i \xi t}\rangle = \int_{\R} f(t)e^{-2\pi i \xi t} dt
\end{equation}
and that we can see as the expansion of a function $f$ in terms of the complex exponentials, elements which represent the eigenfunctions of the one-dimensional Laplace operator:
\begin{equation}
-\Delta(e^{2\pi i \xi t}) = -\frac{\partial^2}{\partial t^2}e^{2\pi i \xi t} = (2\pi\xi)^2e^{2\pi i \xi t}
\end{equation}
From the observation of the classical Fourier Transform, we can analogously define the \textit{Graph Fourier Transform} $\hat{f}$ of any function $f \in \R^N$ on the vertices of $\mathcal{G}$ as the expansion of $f$ in terms of the eigenvectors of the graph Laplacian \cite{Shuman2013}:
\begin{equation}
\hat{f}(\lambda_l) = \langle f, u_l \rangle = \sum^{N}_{i=1}f(i)u_l*(i)
\end{equation}
while, at the same time, the \textit{inverse Graph Fourier Transform} is given by:
\begin{equation}
f(i) = \sum^{N-1}_{l = 0}\hat{f}(\lambda_l)u_l(i)
\end{equation}

\subsection{Representing a graph: graph learning techniques and dictionary representation}
In the previous section we presented the main reason why signal processing field expressed the necessity of having new different structures in order to better represent the amount of information we can collect from a phenomena: what we did not focused on yet, is the fact that these amount of data are usually largely redundant, since they are a densely sampled version of the signal and they may represent multiple correlated versions of the same physical event. So normally the significative information regarding the underlying processes is largely reducible in dimensionality with respect of the collected dataset. \cite{Tosic2011} Thus, we can obtain the data representations starting from the idea that our observations can be described by a sparse subset of elementary signals - so called \textit{atoms} - taken from an \textit{overcomplete dictionary}. When the dictionary forms a basis, then every signal can be univocally represented through the linear combination of the dictionary atoms, moreover the overcompleteness property \cri{definisci i significati delle n. Sei sicura che stai avendo a che fare con dizionari overcomplete? Si si } implies that atoms are linearly dependent. \cite{Tosic2011} \cite{Rubinstein2010}

With these premises, we consider a dictionary $\textbf{D} = [\textbf{d}_1, \textbf{d}_2, \dots, \textbf{d}_L] \in \R^{N \times L}$, in which the columns represent the dictionary atoms and, for the overcompleteness, $L\geq N$. Through this entity, the signal representations can be done through two main paths, either the \textit{synthesis} path, or the \textit{analysis} path, and the two can significantly differ in the overcomplete case.

In the synthesis path, the signal $\textbf{x} \in \R^N$ is represented as a linear combination of the dictionary atoms:
\begin{equation}
\textbf{x} = \textbf{D}^T \gamma_s
\label{eq:synthesis}
\end{equation}
while in the analysis path it is represented through its inner product with the atoms:
\begin{equation}
\gamma_a = \textbf{D}^T \textbf{x}
\label{eq:analysis}
\end{equation}

The representation in \ref{eq:synthesis} has the consequence that, when the dictionary is overcomplete, the set of representations $\gamma_s$satisfying the equation is \textit{infinitely large}, allowing us to look for the most informative representation of the signal with respect of a certain cost function $\mathcal{C}(\gamma)$. In this way we arrive to a first general optimization problem in the form:
\begin{equation}
\gamma_s = \argmin_{\gamma} \text{  } \mathcal{C}(\gamma) \quad \text{Subject To  } \textbf{x} = \textbf{$D\gamma$}
\label{eq:costF}
\end{equation}
The way we choose the form of the cost function obviously influences the structure of our solution, to be specific, one of our goals is to achieve sparsity in the representation of the signal, such that the signal reconstruction is reduced in dimensionality as we are trying to achieve from the beginning. Problem in \ref{eq:costF} becomes what is commonly referred as \textit{sparse coding}, and there are different functions we can apply in order to obtain it: these functions have the characteristic of being tolerant to large coefficients and at the same time importantly penalize small non-zero ones. Among these functions, our choice fell on the $l$ norm, which is one of the simplest and most effective function of this type.

\subsection{Choosing the right dictionary}
 What we did not focused on yet is the choice of the proper Dictionary for out task. In the research there has been so far, different models of Dictionaries have been defined and used for the most different purposes, in the beginning the attention was mainly on traditional dictionaries, such as wavelet and Fourier dictionaries, which are simple to use and perform well for 1-dimensional signals. However, these structures were too simple to properly describe more complex and high-dimensional data, so the focus slowly moved to seeking solutions that better performed in this environment. Dictionaries emerged from this need were coming from two main sources:
 \begin{itemize}
 \item As an \textit{analytical model}, which allostraight form the dataws for a fast implicit implementation that does not require multiplication by the dictionary matrix;
 \item As a \textit{set of realizations} of the data, which offers an increased flexibility and adaptability to data structure;
\end{itemize}
The first model prefers speed over adaptability, since its success depends on the chosen underlying model, sometimes resulting in a over-simplistic solution for the proposed problem. From this, the necessity of also focusing on the second type of dictionary, also defined as \textit{trained dictionary}.\\
Machine learning techniques of the period between 1980's and 1990's allowed to approach this problem under the new assumption that the structure of a natural phenomena can be accurately extracted \textit{straight form the data} with better results than using a mathematical formulation. The most recent training methods focus on the $l^0$ and $l^1$ sparsity measures, which have a simpler formulation and at the same time can use the more recent sparsity coding techniques. \cite{Gorodnitsky1997} \cite{Pati1993}


  \cri{vedi se aggiungere un excursus storico sui dizionari ad apprendimento}



\subsection{The smoothness assumption}
\end{document}
