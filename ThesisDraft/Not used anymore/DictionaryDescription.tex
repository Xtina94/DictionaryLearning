\chapter{Representing a signal: graph learning techniques and dictionary representation}
In \autoref{ch:introduction} we presented the main reason why signal processing field expressed the necessity of having new different structures in order to better represent the amount of information we can collect from a phenomena: what we did not focused on yet, is the fact that these amount of data are usually largely redundant, since they are a densely sampled version of the signal and they may represent multiple correlated versions of the same physical event. So normally the significative information regarding the underlying processes is largely reducible in dimensionality with respect of the collected dataset. \cite{Tosic2011} Thus, we can obtain the data representations starting from the idea that our observations can be described by a sparse subset of elementary signals - so called \textit{atoms} - taken from an \textit{overcomplete dictionary}. When the dictionary forms a basis, then every signal can be univocally represented through the linear combination of the dictionary atoms, moreover the overcompleteness property implies that atoms are linearly dependent. \cite{Tosic2011} \cite{Rubinstein2010}

With these premises, we consider a dictionary $\textbf{D} = [\textbf{d}_1, \textbf{d}_2, \dots, \textbf{d}_L] \in \R^{N \times L}$, in which the columns represent the dictionary atoms and, for the overcompleteness, $L\geq N$. Through this entity, the signal representations can be done through two main paths, either the \textit{synthesis} path, or the \textit{analysis} path, and the two can significantly differ in the overcomplete case.\\
In the synthesis path, the signal $\textbf{x} \in \R^N$ is represented as a linear combination of the dictionary atoms:
\begin{equation}
\textbf{x} = \mathcal{D}^T \gamma_s
\label{eq:synthesis}
\end{equation}
while in the analysis path it is represented through its inner product with the atoms:
\begin{equation}
\gamma_a = \mathcal{D}^T \textbf{x}
\label{eq:analysis}
\end{equation}
Where $\textbf{x}$ accounts for the sparsity concept and is called \textit{sparsity matrix}: as the name suggests, this matrix is a sparse matrix having in its columns a number of non-zero elements equal to the sparsity coefficient we impose. The sparsity coefficients indicates the number of atoms in the dictionary concurring to reproduce the signal in the vertex corresponding to the dictionary row, while the positions of these non-zeros elements in $\textbf{b}$ indicate which are the sources of this generated signal.\\

The representation in \autoref{eq:synthesis} has the consequence that, when the dictionary is overcomplete, the set of representations $\gamma_s$satisfying the equation is \textit{infinitely large}, allowing us to look for the most informative representation of the signal with respect of a certain cost function $\mathcal{C}(\gamma)$. In this way we arrive to a first general optimization problem in the form:
\begin{equation}
\gamma_s = \argmin_{\gamma} \text{  } \mathcal{C}(\gamma) \quad \text{Subject To  } \textbf{x} = \mathcal{D}\textbf{$\gamma$}
\label{eq:costF}
\end{equation}
The way we choose the form of the cost function obviously influences the structure of our solution. One of our goals is to achieve sparsity in the representation of the signal, such that the signal reconstruction is reduced in dimensionality as we are trying to achieve from the beginning. Problem stated in \autoref{eq:costF} becomes what is commonly referred as \textit{sparse coding}, and there are different functions we can apply in order to obtain it: these functions have the characteristic of being tolerant to large coefficients and, at the same time, importantly penalize small non-zero ones. Among these functions, our choice fell on the $l^1$ norm, which is one of the simplest and most effective functions of this type.

\section{Choosing the right dictionary}
 What we did not focused on yet is the choice of the proper dictionary for out task. In the research there has been so far, different models of dictionaries have been defined and used for the most different purposes, in the beginning the attention was mainly on traditional dictionaries, such as wavelet and Fourier dictionaries, which are simple to use and perform well for 1-dimensional signals. However, these structures were too simple to properly describe more complex and high-dimensional data, so the focus slowly moved to seeking solutions that better performed in this environment. Dictionaries emerged from this need were coming from two main sources:
 \begin{itemize}
 \item As an \textit{analytical model}, which allows to extract the dictionary straight form the data for a fast implicit implementation that does not require multiplication by the dictionary matrix;
 \item As a \textit{set of realizations} of the data, which offers an increased flexibility and adaptability to data structure;
\end{itemize}
The first model prefers speed over adaptability, since its success depends on the chosen underlying model, sometimes resulting in a over-simplistic solution for the proposed problem. From this, the necessity to also focus on the second type of dictionary, also defined as \textit{trained dictionary}.\\
Machine learning techniques of the period between 1980's and 1990's allowed to approach this problem under the new assumption that the structure of a natural phenomena can be accurately extracted \textit{straight form the data} with better results than using a mathematical formulation. The most recent training methods focus on the $l^0$ and $l^1$ sparsity measures, which have a simpler formulation and at the same time can use the more recent sparsity coding techniques. \cite{Gorodnitsky1997} \cite{Pati1993}
Among these learning methods, particular relevance acquired the \textit{parametric training methods}, such as \textit{translation-invariant dictionaries}, \textit{multiscale dictionaries} or \textit{sparse dictionaries}. These implementations involve the reduction of parameters' number and the assumption of several desirable properties on the dictionary, in the end leading to an accelerate convergence, reduced density of the local minima and the convergence to a better solution. Moreover, the generalization of the learning process is improved thanks to the smaller number of parameters, as much as the reduction in number of examples needed. Finally, parametric dictionaries bring to a more efficient implementation, due to the fact that parametrization typically has a more compact representation and, not less important, a parametric dictionary may be designed to represent infinite or arbitrary-sized signals. \cite{Rubinstein2010} \cri{forse aggiungi parte sugli sparse dictionaries sempre presa dallo stesso paper. Poi: vedi se aggiungere un excursus storico sui dizionari ad apprendimento}.
\subsubsection{The sparse approximation}
The intention of sparse approximations is to represent a certain signal $y$ of dimension $n$ as the linear combination of a small amount of signals selected from the source database, which is the dictionary. In the dictionary the elements are typically unit norm functions, called \textit{atoms}, that can be denoted through $\phi_k$ (with $k = 1,\dots, N$ and being $N$ the size of the dictionary) and span the entire space the signals live in.\\
When a dictionary is overcomplete, then every signal can be represented with the aforementioned linear combination:
\begin{equation}
\textbf{y} = \textbf{$\phi$}\textbf{a} = \sum_{k=1}^{N}a_k\phi_k
\end{equation}
and the value that \textbf{a} can assume is not unique.\\
To achieve sparse and efficient representations, the requirement for finding the exact representation is in general relaxed: starting from the NP-hard problem which is the minimization of the $l^0$ norm of \textbf{a}, we can approach the issue through convex relaxation methods that solve a problem in the form:
\begin{equation}
\min_\textbf{a} \text{ } (|| \textbf{y} - \textbf{$\phi$a} ||_2^2 + \textbf{$\lambda$}||\textbf{a}||_1)
\end{equation}
in which the relaxation allowed to replace the nonconvex $l^0$ norm in the original problem with a more meaningful $l^1$ norm. \cite{Tosic2011}
