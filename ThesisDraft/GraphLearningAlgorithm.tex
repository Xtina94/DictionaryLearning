\chapter{The graph learning algorithm}
As previously anticipated, our work involved also the need of learning the graph manifold structuring a signal. In order to accomplish it, we made use of the contribution of Maretic et al. to the problem, as it is presented in \cite{Maretic2017}. In the work, a generic model analogous to the one described in \cite {Thanou2014} is considered, where the signals are represented by combinations of local overlapping patterns residing on graphs and where the intrinsic graph structure is merged with the dictionary through the already well known graph Laplacian operator.\\
In the process, the optimization is performed over the weight matrix $W$ instead of $\mathcal{L}$ due to the fact that the constraints defining a valid weight matrix $W$ are less demanding than those defining a valid Laplacian.

\section{The algorithm}
Since also this optimization problem is non convex, Maretic et al. solved it through the alternation of the optimization steps: in the first step a weight matrix $W$ is fixed and the objective function is optimized with respect to the sparsity matrix, using \gls{omp}, in an analogous way than in \cite{Thanou2014}; in the second step the focus is on the graph learning using the previously estimated $X$, and since the relative problem is still non convex, the proposed solution involved an approach based on gradient descent.

The optimization problem addressed as
\begin{align}
\argmin_W \quad &||Y - DX||_F^2 + \beta_W ||W||_1\\
\text{subject to} \quad &\mathcal{D} = [\mathcal{D}_1,\mathcal{D}_2,\dots,\mathcal{D}_S]\\
                        &\mathcal{D} = \sum_{k=0}^{K}\alpha_{sk}\mathcal{L}^k, \qquad \forall s \in \{1,\dots,S\}\\
                        &\mathcal{L} = I - D^{\frac{1}{2}}WD^{\frac{1}{2}}\\
                        &W_{ij} = W_{ji} \geq 0, \qquad \forall i,j \quad i \ne j\\
                        &W_{ij} = 0, \forall i
\end{align}
is then turned into the gradient equivalent form:
\begin{equation}
\begin{split}
&\nabla_W ||Y - \sum_{s=1}^{S}\mathcal{D}_s X_s||_F^2\\
&= \sum_{s=1}^S \sum_{k=1}^K \alpha_{sk} (- \sum_{r = 0}^{k-1}2A^T_{k,r}+\textbf{1}_{N\times N}(B_k \circ I))
\end{split}
\end{equation}

Where the values the matrices $A_{k,r}$ and $B_k$ hold are:
\begin{align}
A_{k,r} &= D^{-\frac{1}{2}} \mathcal{L}^{k-r-1} X_s (Y - DX)^T \mathcal{L}^r D^{-\frac{1}{2}}\\
&and\\
B_k &= \sum_{r=0}{k-1}D^{-\frac{1}{2}} W A_{k,r} D^{-\frac{1}{2}} + A_{k,r} W D^{-1}
\end{align}
% \begin{align}
% A_{k,r} &= D^{-\frac{1}{2}}\mathcal{L}^{k-r-1} X_s (Y - DX)^T \mathcal{L}^r D^{-\frac{1}{2}}\\
%         $and
% B_k     &= \sum_{r=0}^{k-1}D^{-\frac{1}{2}} W A_{k,r}D^{-\frac{1}{2}} + A_{k,r}W D^{-1}
% \end{align}
moreover $\textbf{1}_{N\times N}$ is a matrix of ones, $I$ is an $N\times N$ identity matrix and the gradient of $\beta_W||W||_1$ can be approximated with $\beta_W sign(W)$
